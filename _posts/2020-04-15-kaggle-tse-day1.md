---
keywords: fastai
description: My journey to learn some NLP.
title: Kaggle Tweet Sentiment Extraction
toc: true 
badges: false
comments: true
categories: [kaggle, nlp]
image: images/kaggle_tse/twitter.png
nb_path: _notebooks/2020-04-15-kaggle-tse-day1.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-15-kaggle-tse-day1.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Intro">Intro<a class="anchor-link" href="#Intro"> </a></h1><p>Here lies my notes about my solution for the <a href="https://www.kaggle.com/c/tweet-sentiment-extraction">Kaggle Twitter Sentiment Extraction competition</a>.</p>
<p>First of all, I am still a novice in the field of natural language processing. 
This means that all of these NLP concepts, and even Deep Learning approaches to this fields, are challenging for me to understand and apply. 
Hence, if my approaches somehow become wrong, please give me a notification via my personal email: <a href="huygdng@gmail.com">huygdng@gmail.com</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="About-this-competition">About this competition<a class="anchor-link" href="#About-this-competition"> </a></h2><p>Now let's go to the competition's description. We know that, Sentiment classification is a well-known problem in NLP. 
Given a sentence (a tweet, a line from one book, etc.), our algorithm should be able to tell the "attitude" of that input. 
For example, given a sentence like this:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>Kaggle is fun!</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above sentence is full of "positive" thought, and if I have a proper sentiment classifier, I will expect it to
return "positive" as the "attitude" of that sentence as well.</p>
<p>Now, back to the competition. The challenge in this competition is not to classify the sentiment of tweets, but to <strong>pick out</strong> parts that reflect the sentiment of those tweets.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is from the original description of the competition:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds.
But, which words actually lead to the sentiment description?
In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the above positive example, it is expected from a proper sentiment extraction model to point out the term 'fun!' as the cause of the sentence's positiveness. We can observe that, in this example, not only the word 'fun' is marked as the positive term, but its corresponding punctuation as well. This phenomenon affects the choice of approaches, as we will see later on.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="A-first-glance-at-data">A first glance at data<a class="anchor-link" href="#A-first-glance-at-data"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is some samples from the training dataset:</p>
<p><img src="/huysnote/images/copied_from_nb/../images/kaggle_tse/kaggle_sample.png" alt="kaggle-sample"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, the training data does give us a lot information:</p>
<ul>
<li><p>The information that we could use for our training includes 'text' and 'sentiment'.</p>
</li>
<li><p>The predicion we would make, is the 'selected_text' column.</p>
</li>
<li><p>We have 3 classes for sentiment: positive, negative and neutral.</p>
</li>
<li><p>About 'text' column: The format is quite ...diverge. There are incomplete sentences ('is back home ... '), sentences with some emoticon ('... &lt;3 &lt;3'), sentences with some typos ('Hes just not ...'), and more.</p>
</li>
<li><p>The sentiment distribution is quite good: neutral 40%, positive 31% and negative 28%.</p>
</li>
<li><p>The selected text contains both the word, punctuation and also some emoticon as well.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Listed below are some more basic analysis on this data. We first observe that, the sentiment distribution of training dataset and test dataset are equivalent.</p>
<p><img src="/huysnote/images/copied_from_nb/../images/kaggle_tse/sentiment_dist.png" alt="kaggle-eda1"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our task in this competition, once again, is to find correct pieces of text that emphasize the sentiment of the tweet. Hence, observing the distribution of word counts in both the original tweet the selected tweet (the target) in each sentiment class is a good idea.</p>
<p><img src="/huysnote/images/copied_from_nb/../images/kaggle_tse/text_length_comparison1.png" alt="kaggle-eda2"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also the corresponding histogram of word counts.</p>
<p><img src="/huysnote/images/copied_from_nb/../images/kaggle_tse/text_length_comparison2.png" alt="kaggle-eda1"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Among the 3 sentiment classes, the 'neutral' class has one interesting characteristics: The length of input tweet and target piece of text are almost the same. Hence, we can make use of it, as a simple heuristic post-process rule.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="My-approaches">My approaches<a class="anchor-link" href="#My-approaches"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Viewing-the-problem">Viewing the problem<a class="anchor-link" href="#Viewing-the-problem"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This problem can be treated as <strong>Token classification</strong> problem (i.e., Name Entity Recognition, Part-of-Speech, ...) , or <strong>Question-answering</strong> problem.</p>
<p>However, when working with this problem as the token classification problem, we do not include punctuations as part of the model's results. Moreover, modifying token with punctuations is not a good idea - not only does that approach expand the vocabulary (which goes along with time and computational resources to learn the language model), but it also does not guarantee that we can learn the similarity between word and word with tokens. Hence, this approach is not optimal.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Question-answering-problem">The Question-answering problem<a class="anchor-link" href="#The-Question-answering-problem"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I formulated this task as question answering problem: given a question and a context, it is expected that the model should find acceptable  the 
given a question and a context, we train a transformer model to find the answer in the text column (the context).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Models">Models<a class="anchor-link" href="#Models"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Up to now, I have tried some model architectures:</p>
<ul>
<li><p>The pre-trained BERT[<a href="https://arxiv.org/abs/1810.04805">1</a>] with custom header.</p>
</li>
<li><p>The pre-trained Electra[<a href="https://arxiv.org/abs/2003.10555">2</a>] with custom header.</p>
</li>
<li><p>The pre-trained RoBERTa[<a href="https://arxiv.org/abs/1907.11692">3</a>] with custom header</p>
</li>
</ul>
<p>The custom header includes 2 Linear layers, with ReLU and Dropout for the first linear layer. The header's weights are initialized using Kaiming He normal initialization [<a href="https://arxiv.org/abs/1502.01852">4</a>].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Input-data">Input data<a class="anchor-link" href="#Input-data"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>[1] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>.</p>
<p>[2] Clark, K., Luong, M.T., Le, Q.V. and Manning, C.D., 2020. Electra: Pre-training text encoders as discriminators rather than generators. <em>arXiv preprint arXiv:2003.10555</em>.</p>
<p>[3] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V., 2019. Roberta: A robustly optimized bert pretraining approach. <em>arXiv preprint arXiv:1907.11692</em>.</p>
<p>[4] He, K., Zhang, X., Ren, S. and Sun, J., 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. <em>In Proceedings of the IEEE international conference on computer vision</em> (pp. 1026-1034).</p>

</div>
</div>
</div>
</div>
 

